---
title: "Land use and microbe prevalence jointly determine host-microbe network structure"
subtitle: "Analysis and results"
author: "Matan Markfeld"
date: "Last edit: 2024-07-22"
output: 
  html_document:
    toc: true
    toc_float: true
    collapse: false
    number_sections: true
    code_folding: hide
    highlight: tango
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(results = "asis", message=FALSE, warning=FALSE, cache=TRUE, eval = TRUE, dev = c('png'), out.width = '100%', out.height='60%')
```


```{r load libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(magrittr)
library(ggplot2)
library(vegan)
library(igraph)
library(reshape2)
library(infomapecology)
library(aricode)
library(ape)
library(picante)
library(patchwork)

rm(list=ls())
```

```{r}
source("modularity_analysis_functions.R", local = knitr::knit_global())
```

```{r, microbiome data}
# reading the microbiome data
# working only with Rattus from the three villages

# villages: Andatsakala, Mandena, Sarahandrano
vil <- "Mandena"
data_asv <- read_csv("../data/data_processed/microbiome/data_asv_rra0.001_p0.01_th5000.csv") %>% filter(village == vil)
```

```{r, main analysis, include=FALSE, results='hide'}
# first of all, run this chunk of code.
# this code does all the modularity analysis using the functions in "modularity_analysis_functions".
# There is a loop runs for the three villages, and all the analysis is done separately for each village.
# the output of the code is variables holding the final results tables and figures (the output of the functions used)

# setting thresholds for core
core_th <- c(0.02, 0.2)
n_phylo_shuff <- 10
data_asv %<>% mutate(asv_core = case_when(host_p<core_th[1] ~ "Rare",
                                          host_p>core_th[2] ~ "Core",
                                          .default = "Non-core"))

group.colors <- c(Core = "#f4a261", 'Non-core' = "#2b9348", Rare = "#0077b6")
#group.colors <- c(Core = "#e76f51", Non_core = "#2b9348", Rare = "#0077b6")

core_names <- unique(data_asv$asv_core)
nmi_observed_three_groups <- NULL
modules_similarity_three_groups <- NULL
asv_degree_distribution_three_groups <- NULL
modules_three_groups <- NULL
modules_size_three_groups <- NULL
modules_grid_three_groups <- NULL
modules_table_three_groups <- NULL
betaNTI_three_groups <- NULL
raupc_three_groups <- NULL

# reading the phylogenetic tree
best_tree <- readRDS(file = "../results/phylo_tree_rra0.001_p0.01.rds")
phylo_tree <- best_tree$tree 
# ASVs phylogenetic distance
asv_distance <- ape::cophenetic.phylo(phylo_tree)

# loop for three groups
for (v in core_names) {
  
  data_asv_group <- data_asv %>% 
    filter(asv_core == v)
  
  # calculating ASVs degree
  asv_degree <- data_asv_group %>%
    group_by(asv_ID) %>%
    summarise(n = n_distinct(host_ID)) %>%
    dplyr::rename(asv_degree = n)
  data_asv_group %<>% left_join(asv_degree, by="asv_ID")
  
  ##### observed network
  # finding modules
  modules_observed <- fun_modularity_analysis(data_asv_group)
  modules_table_three_groups <- rbind(modules_table_three_groups, modules_observed)
  #write_csv(modules_table_three_groups, "../results/modules_table_sarahandrano.csv")
  
  # calculating similarity in modules between grids
  modules_similarity <- fun_modules_similarity(modules_observed)
  # saving results in one table
  modules_similarity_three_groups <- append(modules_similarity_three_groups, modules_similarity)
  
  # ASVs degree distribution
  asv_degree_distribution <- fun_asv_degree_distribution(data_asv_group)
  asv_degree_distribution_three_groups <- append(asv_degree_distribution_three_groups, asv_degree_distribution)
  
  # figure of modules 
  modules <- fun_modules(modules_observed)
  modules_three_groups <- append(modules_three_groups, modules)
  
  # modules size
  modules_size <- fun_module_size(modules_observed)
  modules_size_three_groups <- append(modules_size_three_groups, modules_size)
  
  # Number of land uses per module
  modules_grid <- fun_module_grid(modules_observed)
  modules_grid_three_groups <- append(modules_grid_three_groups, modules_grid)
  
  # calculating NMI
  nmi_observed <- fun_nmi_calc(modules_observed, TRUE)
  # saving results in one list
  nmi_observed_three_groups <- append(nmi_observed_three_groups, nmi_observed)
  
  
  ##### phylogenetic analysis
  # ASVs pool
  asv_pool <- data_asv_group %>% 
    distinct(asv_ID, asv_degree) %>% 
    mutate(p = asv_degree/length(unique(data_asv_group$asv_ID)))
  
  # calculating betaNTI
  data_betaNTI <- data_asv_group %>% 
    distinct(host_ID, asv_ID) %>% 
    mutate(reads = 1) %>% 
    spread(asv_ID, reads, fill = 0) %>% 
    column_to_rownames("host_ID") %>% 
    as.matrix()
  
  betaNTI <- fun_calc_betaNTI(data_betaNTI, asv_distance, asv_pool, n_phylo_shuff) %>% 
    left_join(modules_observed %>% distinct(host_ID, host_group, grid), by=c("host1"="host_ID")) %>% dplyr::rename(host_group1=host_group, grid1=grid) %>% 
    left_join(modules_observed %>% distinct(host_ID, host_group, grid), by=c("host2"="host_ID")) %>% dplyr::rename(host_group2=host_group, grid2=grid) %>% 
    mutate(asv_core = v)
  betaNTI_three_groups <- rbind(betaNTI_three_groups, betaNTI)
  
  # raup-crick
  raupc <- raupcrick(data_betaNTI)
  raupc2 <- as.matrix(raupc) 
  raupc2[upper.tri(raupc2)] <- NA
  diag(raupc2) <- NA
  raupc2_m <- melt(raupc2) %>% 
    filter(!(is.na(value)))%>% 
    dplyr::rename(host1 = Var1, host2 = Var2, raupc = value) %>% 
    left_join(modules_observed %>% distinct(host_ID, host_group, grid), by=c("host1"="host_ID")) %>% dplyr::rename(host_group1=host_group, grid1=grid) %>% 
    left_join(modules_observed %>% distinct(host_ID, host_group, grid), by=c("host2"="host_ID")) %>% dplyr::rename(host_group2=host_group, grid2=grid) %>% 
    mutate(asv_core = v)
  raupc_three_groups <- rbind(raupc_three_groups, raupc2_m)
}
```

# Exploration {.tabset}

```{r, sm exploration}
# small mammals data
small_mammals <- read_csv("../data/data_raw/data_small_mammals/Terrestrial_Mammals.csv") %>% 
  mutate(host_ID = as.numeric(gsub(".*?([0-9]+).*", "\\1", animal_id))) %>% 
  mutate(age_repro = as_factor(age_repro)) %>% 
  dplyr::rename(grid = habitat_type) %>% 
  filter(host_ID %in% data_asv$host_ID)

cat("## Abundance", '\n','\n')
g <- small_mammals %>% 
  count(grid) %>% 
  ggplot(aes(x=grid, y=n)) +
  geom_bar(position="dodge", stat="identity") +
  theme_bw() +
  theme(axis.text = element_text(size = 10, color = 'black', angle = 90, vjust = 0.5, hjust=1), title = element_text(size = 14)) +
  labs(x="Land Use", y="Small-Mammals Abundance")
print(g)

cat("## Sex ratio", '\n','\n')
g <- small_mammals %>% 
  count(grid, sex) %>% 
  ggplot(aes(fill=sex, x=grid, y=n)) +
  geom_bar(position="fill", stat="identity") +
  geom_hline(yintercept = 0.5, linetype = "dashed") +
  theme_bw() +
  theme(axis.text = element_text(size = 10, color = 'black', angle = 90, vjust = 0.5, hjust=1), title = element_text(size = 14)) +
  labs(x="Land Use", y="Proportion")
print(g)

cat("## Age ratio", '\n','\n')
g <- small_mammals %>% 
  count(grid, age_repro) %>% 
  ggplot(aes(fill=age_repro, x=grid, y=n)) +
  geom_bar(position="fill", stat="identity") +
  theme_bw() +
  theme(axis.text = element_text(size = 10, color = 'black', angle = 90, vjust = 0.5, hjust=1), title = element_text(size = 14)) +
  labs(x="Land Use", y="Proportion")
print(g)
```

```{r, microbiome exploration}
# relative abundance of the 10 most abundant Family of each group

# microbes taxonomy
tax <- read_delim("../data/data_raw/data_microbiome/ASVs_taxonomy_new.tsv") %>% 
  dplyr::rename(asv_ID = ASV)

data_asv_tax <- data_asv %>% 
  left_join(tax, by="asv_ID")

cat("## Relative abundance Family", '\n','\n')
total_reads_groups <- data_asv_tax %>% distinct(host_ID, asv_core, total_reads) %>% group_by(asv_core) %>% summarise(n_total = sum(total_reads))
most_abu_family <- data_asv_tax %>%
  mutate(reads_a = reads*total_reads) %>%
  group_by(asv_core, Family) %>%
  summarise(n= sum(reads_a)) %>%
  left_join(total_reads_groups, by="asv_core") %>%
  mutate(n_p = n/n_total) 

  most_abu_family_8 <- most_abu_family %>% 
    filter(!is.na(Family)) %>% 
    ungroup() %>% 
    slice_max(by = asv_core, order_by = n_p, n = 10) %>% 
    mutate(p = "top") %>% 
    select(asv_core, Family, p)
  
  most_abu_family_p <- data_asv_tax %>%
  mutate(reads_a = reads*total_reads) %>%
  group_by(asv_core, Family, grid) %>%
  summarise(n= sum(reads_a)) %>%
  left_join(total_reads_groups, by="asv_core") %>%
  mutate(n_p = n/n_total) %>% 
    left_join(most_abu_family_8, by=c("asv_core","Family")) %>% 
    mutate(p = case_when(p=="top" ~ Family,
                         is.na(Family) ~ ".NA",
                         is.na(p) ~ ".Other"))
library("RColorBrewer")
colors <- c("grey40","grey80","darkblue","#ffd60a",brewer.pal(n = 12, name = "Paired"))
  g <- most_abu_family_p %>% 
    group_by(asv_core,grid,  p) %>% 
    summarise(n_p = sum(n_p)) %>% 
ggplot(aes(fill=p, x=grid, y=n_p)) +
  geom_bar(position="fill", stat="identity") +
  facet_wrap(~asv_core) +
  theme_bw() +
  theme(axis.text = element_text(size = 10, color = 'black', angle = 90, vjust = 0.5, hjust=1), title = element_text(size = 14),
        strip.text = element_text(size=12, color = 'black'), 
        panel.grid = element_blank(), panel.background = element_rect(colour = "black"), legend.key.size = unit(0.5, "cm")) +
  scale_fill_manual(values=colors) +
  labs(x="Land Use", y="Relative Abundance", fill = "Family")
print(g)
cat('\n','\n')
```

# PERMANOVA {.tabset}

## Bray-Curtis
```{r}
set.seed(123)
data_asv_mat <- data_asv %>% 
  #filter(asv_core == "Rare") %>% 
  select(host_ID, asv_ID, reads) %>% 
  spread(asv_ID, reads, fill = 0) %>% 
  column_to_rownames("host_ID") %>% 
  as.matrix()

hosts <- small_mammals %>% 
  filter(host_ID %in% rownames(data_asv_mat)) %>% 
  select(host_ID,grid,season,month,mass, sex,age_repro,age_dental,elevation.obs) %>% 
  mutate(grid=as.factor(grid), season=as.factor(season),month=as.factor(month), sex=as.factor(sex), age_dental=as.factor(age_dental))

distance_matrix <- vegdist(data_asv_mat, method = "bray")

# Perform PERMANOVA
permanova_result <- adonis2(distance_matrix ~ grid+season+sex+age_dental, data = hosts, permutations = 999)
print(knitr::kable(permanova_result[,4:5]))
cat('\n')

# PERMANOVA post-hoc
# perm_post_grid <- RVAideMemoire::pairwise.perm.manova(distance_matrix, fact = hosts$grid, 
#     test = "bonferroni", nperm = 999, progress = FALSE)$p.value %>% as.data.frame()
# print(knitr::kable(perm_post_grid))
# cat('\n')

#NMDS
nmds_result <- metaMDS(distance_matrix, distance = "bray", k = 2, trace = F)

# preparing data for plotting
nmds_plot <- nmds_result$points %>% 
  as.data.frame() %>%  
  rownames_to_column("host_ID") %>% 
  mutate(host_ID = as.double(host_ID)) %>% 
  left_join(hosts, by="host_ID")

# plotting
g1 <- nmds_plot %>% 
  ggplot( aes(MDS1, MDS2, color=grid, shape=season)) +
  geom_point(size = 2, position=position_jitter(.01)) +
  #stat_ellipse(aes(fill=grid), alpha=.1, type='norm',linetype =2, geom="polygon") + ##draws 95% confidence interval ellipses
  theme_bw() +
  annotate("text", x=min(nmds_plot$MDS1)+0.04, size=3, y=max(nmds_plot$MDS2), label=paste('Stress =',round(nmds_result$stress,3))) +
  labs(x = "NMDS1", y = "NMDS2", title = "(A) Bray-Curtis")

```

## UniFrac
```{r}
set.seed(123)
library(GUniFrac)

rooted_phylo_tree <- phangorn::midpoint(phylo_tree)
distance_matrix <- GUniFrac(data_asv_mat, rooted_phylo_tree, verbose = F)$unifracs[, , "d_UW"]

# Perform weighted UniFrac
unifrac_result <- adonis2(distance_matrix ~ grid+season+sex+age_dental, data = hosts, permutations = 999)
print(knitr::kable(unifrac_result[,4:5]))
cat('\n')

# PERMANOVA post-hoc
# perm_post_grid <- RVAideMemoire::pairwise.perm.manova(distance_matrix, fact = hosts$grid, 
#     test = "bonferroni", nperm = 999, progress = FALSE)$p.value %>% as.data.frame()
# print(knitr::kable(perm_post_grid))
# cat('\n')

#NMDS
nmds_result2 <- metaMDS(distance_matrix, k = 2, trace = F)

# preparing data for plotting
nmds_plot2 <- nmds_result2$points %>% 
  as.data.frame() %>%  
  rownames_to_column("host_ID") %>% 
  mutate(host_ID = as.double(host_ID)) %>% 
  left_join(hosts, by="host_ID")

# plotting
g2 <- nmds_plot2 %>% 
  ggplot( aes(MDS1, MDS2, color=grid, shape=season)) +
  geom_point(size = 2, position=position_jitter(.05)) +
  #stat_ellipse(aes(fill=grid), alpha=.1, type='norm',linetype =2, geom="polygon") + ##draws 95% confidence interval ellipses
  theme_bw() +
  annotate("text", x=min(nmds_plot2$MDS1)+0.15, size=3, y=max(nmds_plot2$MDS2), label=paste('Stress =',round(nmds_result2$stress,3))) +
  labs(x = "NMDS1", y = "NMDS2", title = "(B) Weighted UniFrac")

```

## Figure 
```{r, permanova}
# plotting
g1 + g2 + plot_layout(guides='collect') &
  theme(legend.position='bottom', legend.spacing.x=unit(0.1, 'cm'))
cat('\n','\n')
```

# Microbial Groups {.tabset}

## Genus

```{r, taxonomy_genus}
# taking only ASVs with known Genus
data_tax <- data_asv_tax %>% 
  distinct(host_ID, asv_core, Genus) %>% 
  filter(!is.na(Genus))

# known ASVs
asv_genus <- data_asv_tax %>% 
  distinct(asv_ID, Genus) %>%
  mutate(name = is.na(Genus)) 
  print(paste("Percentege of known ASVs:", round(table(asv_genus$name)[1]/nrow(asv_genus)*100, 2)))
  cat('\n')
# how many Genus?
print(paste("Number of genera:", length(unique(data_tax$Genus))))
cat('\n')

# making unique samples (host_ID+asv_core)
data_tax_meta <- data_tax %>% 
  arrange(host_ID) %>% 
  unite(col="sample", host_ID:asv_core, remove = FALSE)
  
data_tax_meta2 <- data_tax_meta %>% distinct(sample, asv_core) 

data_tax_summary <- data_tax %>% 
  count(asv_core, Genus) %>% 
  spread(Genus, n, fill = 0)

# transforming to matrix
data_tax_mat <- data_tax_meta %>% 
  select(sample, Genus) %>% 
  mutate(n=1) %>% 
  spread(Genus, n, fill = 0) %>% 
  column_to_rownames("sample") %>% 
  as.matrix()

# Perform PERMANOVA
permanova_result <- adonis2(data_tax_mat ~ asv_core, data = data_tax_meta2, method = "jaccard", permutations = 999)
print(paste("F =", permanova_result$F[1]))
cat('\n')
print(paste("p-value =", permanova_result$`Pr(>F)`[1]))
cat('\n')

# PCA
tax_pca <- prcomp(data_tax_mat)

# explained variance
ex_var <- tax_pca$sdev ^2 
prop_ex_var <- ex_var/sum(ex_var)*100

loadings <- as.data.frame(tax_pca$rotation[, 1:2]) %>% rownames_to_column("Genus") %>% 
  mutate(PC1_abs = abs(PC1), PC2_abs = abs(PC2))

tax_scores <- as.data.frame(tax_pca$x[,1:2]) %>% rownames_to_column("sample") %>% 
  left_join(data_tax_meta2, by="sample") 

loading_top <- loadings %>% 
  slice_max(n = 5, order_by = PC1_abs) %>% 
  bind_rows(loadings %>% slice_max(n = 5, order_by = PC2_abs)) 

g1 <- tax_scores %>% 
  ggplot(aes(PC1, PC2, color=asv_core)) +
  geom_point() +
  geom_segment(data = loading_top, inherit.aes = FALSE, aes(x = 0, y = 0, xend = PC1*6, yend = PC2*6), 
                 arrow = arrow(length = unit(0.15, "cm"))) +
  annotate("label", x = (loading_top$PC1*6), y = (loading_top$PC2*6),
     label = loading_top$Genus, size=3, label.size=0.1, label.padding=unit(0.05, "lines")) +
  theme_bw() +
  theme(panel.border = element_rect(colour = "black", size=1))+
  scale_fill_manual(values=group.colors) +
  labs(x = paste("PC1 (",round(prop_ex_var[1],2),"%)", sep = ""), y = paste("PC2 (",round(prop_ex_var[2],2),"%)", sep = ""), 
       title = "(A) Genus", color = "ASV Group")
```

## Family

```{r, taxonomy_family}
# taking only ASVs with known Family
data_tax <- data_asv_tax %>% 
  distinct(host_ID, asv_core, Family) %>% 
  filter(!is.na(Family))

# known ASVs
asv_genus <- data_asv_tax %>% 
  distinct(asv_ID, Family) %>%
  mutate(name = is.na(Family)) 
  print(paste("Percentege of known ASVs:", round(table(asv_genus$name)[1]/nrow(asv_genus)*100, 2)))
  cat('\n')
# how many Family?
print(paste("Number of families:", length(unique(data_tax$Family))))
cat('\n')

# making unique samples (host_ID+asv_core)
data_tax_meta <- data_tax %>% 
  arrange(host_ID) %>% 
  unite(col="sample", host_ID:asv_core, remove = FALSE)
  
data_tax_meta2 <- data_tax_meta %>% distinct(sample, asv_core) 

data_tax_summary <- data_tax %>% 
  count(asv_core, Family) %>% 
  spread(Family, n, fill = 0)

# transforming to matrix
data_tax_mat <- data_tax_meta %>% 
  select(sample, Family) %>% 
  mutate(n=1) %>% 
  spread(Family, n, fill = 0) %>% 
  column_to_rownames("sample") %>% 
  as.matrix()

# Perform PERMANOVA
permanova_result <- adonis2(data_tax_mat ~ asv_core, data = data_tax_meta2, method = "jaccard", permutations = 999)
print(paste("F =", permanova_result$F[1]))
cat('\n')
print(paste("p-value =", permanova_result$`Pr(>F)`[1]))
cat('\n')

# PCA
tax_pca <- prcomp(data_tax_mat)

# explained variance
ex_var <- tax_pca$sdev ^2 
prop_ex_var <- ex_var/sum(ex_var)*100

loadings <- as.data.frame(tax_pca$rotation[, 1:2]) %>% rownames_to_column("Family") %>% 
  mutate(PC1_abs = abs(PC1), PC2_abs = abs(PC2))

tax_scores <- as.data.frame(tax_pca$x[,1:2]) %>% rownames_to_column("sample") %>% 
  left_join(data_tax_meta2, by="sample") 

loading_top <- loadings %>% 
  slice_max(n = 5, order_by = PC1_abs) %>% 
  bind_rows(loadings %>% slice_max(n = 5, order_by = PC2_abs)) 

g2 <- tax_scores %>% 
  ggplot(aes(PC1, PC2, color=asv_core)) +
  geom_point() +
  geom_segment(data = loading_top, inherit.aes = FALSE, aes(x = 0, y = 0, xend = PC1*4, yend = PC2*4), 
                 arrow = arrow(length = unit(0.15, "cm"))) +
  annotate("label", x = (loading_top$PC1*4), y = (loading_top$PC2*4),
     label = loading_top$Family, size=3, label.size=0.1, label.padding=unit(0.05, "lines")) +
  scale_x_continuous(limits = c(-2, 2.7)) +
  theme_bw() +
  theme(panel.border = element_rect(colour = "black", size=1))+
  scale_fill_manual(values=group.colors) +
  labs(x = paste("PC1 (",round(prop_ex_var[1],2),"%)", sep = ""), y = paste("PC2 (",round(prop_ex_var[2],2),"%)", sep = ""), 
       title = "(B) Family", color = "ASV Group")
```

## Figure

```{r, taxonomy}
# plotting
g1 + g2 + plot_layout(guides='collect') &
  theme(legend.position='bottom')
cat('\n','\n')
```

# Network exploration {.tabset}

```{r, network_exploration}

# loop for three groups
for (i in 1:3) {
  
  cat('##',core_names[i],'{.tabset}','\n','\n')
  
  cat('### ASVs degree distribution','\n')
  
  print(asv_degree_distribution_three_groups[[i]])
  cat('\n','\n')
  
  # calculating connectance
  connectance_data <- modules_table_three_groups %>% 
    filter(asv_core == core_names[i])
  
  cat('No. of hosts: ', length(unique(connectance_data$host_ID)) ,'\n','\n')
  cat('No. of ASVs: ', length(unique(connectance_data$asv_ID)) ,'\n','\n')
  cat('Connectance: ', nrow(connectance_data) / (length(unique(connectance_data$host_ID)) * length(unique(connectance_data$asv_ID))) ,'\n','\n')
  
  
  cat('### Modules','\n')
  cat('The color indicates number of host individuals in the module / total number of hosts in the whole grid [%]','\n','\n')
  
  print(modules_three_groups[[i]])
  cat('\n','\n')
  
  cat('### Modules size','\n')
  print(modules_size_three_groups[[i]])
  cat('\n','\n')
  
  cat('### No. of land uses','\n')
  print(modules_grid_three_groups[[i]])
  cat('\n','\n')
  
}
```

# Modular Structure Exploration {.tabset}

## Modules size

```{r}
library(ggsignif)
#library(ggbreak)

modules_sizes <- modules_table_three_groups %>% 
  group_by(asv_core, host_group) %>% 
  summarise(n = n_distinct(host_ID)) 

# summary
cat('Summary','\n')
modules_size_summary <- modules_sizes %>% 
  group_by(asv_core) %>% 
  summarise(mean = mean(n), sd = sd(n))
print(knitr::kable(modules_size_summary))
cat('\n')
# ANOVA between groups
cat('ANOVA','\n')
anova_result <- aov(n ~ asv_core, data = modules_sizes)
print(paste("F =", summary(anova_result)[[1]][["F value"]][1]))
cat('\n')
print(paste("p-value =", summary(anova_result)[[1]][["Pr(>F)"]][1]))
cat('\n')

# Perform Tukey's HSD post hoc test
cat('Tukey post-hoc','\n')
tukey_result <- TukeyHSD(anova_result)
# Extract the test statistics (q values) from the Tukey HSD results
tukey_q <- tukey_result$asv_core[, "diff"] / tukey_result$asv_core[, "lwr"]
tukey_result <- cbind(tukey_result$asv_core, q_value = tukey_q) %>% 
  as.data.frame() %>% 
  rownames_to_column("comp")
print(knitr::kable(tukey_result))
cat('\n')
 

  g1 <- modules_sizes %>% 
  ggplot(aes(x=asv_core, y=n, fill=asv_core)) + 
  geom_boxplot(outliers = FALSE, alpha = 0.9) +
  geom_jitter(color="black", size=1) +
  theme_classic() +
  #scale_y_continuous(limits = c(0, 145)) +
  #scale_y_break(c(50,130), space=0.4, ticklabels = c(130,140))+
  geom_signif(comparisons = list(c("Core", "Non-core"),c("Core", "Rare"),c("Non-core", "Rare")), 
              annotations = c("***","***","ns"),  y_position = c(90,100,80), tip_length = 0) +
  geom_point(data = modules_size_summary, aes(x=asv_core, y = mean), position = position_dodge(width = 0.75), size = 2, color="#d62828",alpha=0.8, show.legend=F) +
  theme(axis.text = element_text(size = 11, color = 'black'), title = element_text(size = 15), legend.position = "none") +
  scale_fill_manual(values=group.colors) +
  labs(x="ASV Group", y="Module Size", title = "(A)")
```

## Modules No. of Grids

```{r}
modules_grids <- modules_table_three_groups %>% 
  group_by(asv_core, host_group) %>% 
  summarise(n = n_distinct(grid)) 

# summary
cat('Summary','\n')
modules_grids_summary <- modules_grids %>% 
  group_by(asv_core) %>% 
  summarise(mean = mean(n), sd = sd(n))
print(knitr::kable(modules_grids_summary))
cat('\n')
# ANOVA between groups
cat('ANOVA','\n')
anova_result <- aov(n ~ asv_core, data = modules_grids)
print(paste("F =", summary(anova_result)[[1]][["F value"]][1]))
cat('\n')
print(paste("p-value =", summary(anova_result)[[1]][["Pr(>F)"]][1]))
cat('\n')

# Perform Tukey's HSD post hoc test
cat('Tukey post-hoc','\n')
tukey_result <- TukeyHSD(anova_result)
# Extract the test statistics (q values) from the Tukey HSD results
tukey_q <- tukey_result$asv_core[, "diff"] / tukey_result$asv_core[, "lwr"]
tukey_result <- cbind(tukey_result$asv_core, q_value = tukey_q) %>% 
  as.data.frame() %>% 
  rownames_to_column("comp")
print(knitr::kable(tukey_result))
cat('\n')

  g2 <- modules_grids %>% 
  ggplot(aes(x=asv_core, y=n, fill=asv_core)) + 
  geom_boxplot(outliers = FALSE, alpha = 0.9) +
  geom_jitter(color="black", size=1) +
  theme_classic() +
  scale_y_continuous(limits = c(0, 9), breaks = seq(0,8,by=2)) +
  geom_signif(comparisons = list(c("Core", "Non-core"),c("Core", "Rare"),c("Non-core", "Rare")), 
              annotations = c("***","***","ns"),  y_position = c(7.7,8.5,7), tip_length = 0) +
  geom_point(data = modules_grids_summary, aes(x=asv_core, y = mean), position = position_dodge(width = 0.75), size = 2, color="#d62828",alpha=0.8, show.legend=F) +
  theme(axis.text = element_text(size = 11, color = 'black'), title = element_text(size = 15), legend.position = "none") +
  scale_fill_manual(values=group.colors) +
  labs(x="ASV Group", y="No. of Land Uses", title = "(B)")
```

## No. of Modules

```{r}
modules_per_grid <- modules_table_three_groups %>% 
  group_by(asv_core, grid) %>% 
  summarise(n = n_distinct(host_group)) 

# summary
cat('Summary','\n')
modules_per_grid_summary <- modules_per_grid %>% 
  group_by(asv_core) %>% 
  summarise(mean = mean(n), sd = sd(n))
print(knitr::kable(modules_per_grid_summary))
cat('\n')
# ANOVA between groups
cat('ANOVA','\n')
anova_result <- aov(n ~ asv_core, data = modules_per_grid)
print(paste("F =", summary(anova_result)[[1]][["F value"]][1]))
cat('\n')
print(paste("p-value =", summary(anova_result)[[1]][["Pr(>F)"]][1]))
cat('\n')

# Perform Tukey's HSD post hoc test
cat('Tukey post-hoc','\n')
tukey_result <- TukeyHSD(anova_result)
# Extract the test statistics (q values) from the Tukey HSD results
tukey_q <- tukey_result$asv_core[, "diff"] / tukey_result$asv_core[, "lwr"]
tukey_result <- cbind(tukey_result$asv_core, q_value = tukey_q) %>% 
  as.data.frame() %>% 
  rownames_to_column("comp")
print(knitr::kable(tukey_result))
cat('\n')

  g3 <- modules_per_grid %>% 
  ggplot(aes(x=asv_core, y=n, fill=asv_core)) + 
  geom_boxplot(outliers = FALSE, alpha = 0.9) +
  geom_jitter(color="black", size=1) +
  theme_classic() +
  scale_y_continuous(limits = c(0, 60)) +
  geom_signif(comparisons = list(c("Core", "Non-core"),c("Core", "Rare"),c("Non-core", "Rare")), 
              annotations = c("***","***","ns"),  y_position = c(45,55,50), tip_length = 0) +
  geom_point(data = modules_per_grid_summary, aes(x=asv_core, y = mean), position = position_dodge(width = 0.75), size = 2, color="#d62828",alpha=0.8, show.legend=F) +
  theme(axis.text = element_text(size = 11, color = 'black'), title = element_text(size = 15), legend.position = "none") +
  scale_fill_manual(values=group.colors) +
  labs(x="ASV Group", y="No. of Modules per Land Use", title = "(C)")
```

## Figure

```{r, modules_exploration}
# plotting
g1+g2+g3 + plot_layout(axis_titles = "collect_x")
```


```{r, groups_modules, eval=FALSE, include=FALSE}
host_module_partners <- function(dat, hosts) {
  
  group_similarity_all <- NULL
  for (h in hosts) {
    
    # the modules of the focal host
    h_module <- dat %>% filter(host_ID==h) %>% select(-host_ID) %>% mutate(s=T)
    # taking only the module partners of the focal host
    dat_module <- dat %>% 
      left_join(h_module, by=c("asv_core","host_group")) %>% 
      filter(s) %>% 
      select(asv_core, host_ID) %>% 
      mutate(n=1, host_ID = as.character(host_ID)) %>% 
      spread(host_ID, n, fill=0) %>% 
      column_to_rownames("asv_core") %>% 
      as.matrix()
    
    # removing the focal host from the table
    #dat_module <- dat_module[, colnames(dat_module)!=h]
    
    # calculating similarity
    group_similarity <- as.matrix(1-vegdist(dat_module, method = "jaccard"))
    group_similarity[upper.tri(group_similarity)] <- NA
    diag(group_similarity) <- NA
    group_similarity_m <- melt(group_similarity) %>% 
    filter(!(is.na(value))) %>% 
      mutate(host_ID = h)
    
    group_similarity_all <- rbind(group_similarity_all, group_similarity_m)
  }
  
  # calculating the mean
  group_similarity_mean <- group_similarity_all %>% 
    unite("groups", Var1:Var2) %>%
    group_by(groups) %>% 
    summarise(mean = mean(value), median = median(value), sd = sd(value))
    
  return(group_similarity_mean)
}


host_modules <- modules_table_three_groups %>% 
  distinct(asv_core, host_ID, host_group)

host_list <- unique(host_modules$host_ID)

# observed results
groups_sim_obs <- host_module_partners(host_modules, host_list)

# shuffle modules membership
groups_sim_shuff_all <- NULL
for(i in 1:20) {
host_modules_shuff <- host_modules %>% 
      group_by(asv_core) %>%
      mutate(host_group = sample(host_group))

groups_sim_shuff <- host_module_partners(host_modules_shuff, host_list)
groups_sim_shuff_all <- rbind(groups_sim_shuff_all, groups_sim_shuff)
}

# plotting
g <- groups_sim_shuff_all %>% 
  ggplot(aes(x=mean, group=groups, fill=groups)) +
    geom_histogram() +
  geom_vline(data=groups_sim_obs, aes(xintercept = mean), linetype='dashed', color="black") +
    facet_wrap(~groups) +
   theme_bw() +
    theme(
      legend.position="none",
      panel.spacing = unit(0.1, "lines")
    )
print(g)


hosts <- modules_table_three_groups %>% distinct(host_ID, asv_core, host_group) %>% 
  spread(asv_core, host_group) %>% filter(across(everything(), ~!is.na(.)))
```

# Phylogenetic analysis

```{r}

# beta-NTI same module?
betaNTI_final <- betaNTI_three_groups %>% 
  mutate(same_module = ifelse(host_group1==host_group2, "Same","Different")) 

# Raup-Crick same module?
raupc_final <- raupc_three_groups %>% 
  mutate(same_module = ifelse(host_group1==host_group2, "Same","Different"))

```

```{r, assembly_processes}
# combining final results
assembly_final <- betaNTI_final %>% 
  full_join(raupc_final %>% dplyr::select(host1,host2,raupc,asv_core), by=c("host1","host2","asv_core"))

#a=assembly_final %>% filter(is.na(betaNTI))

# calculating the process
assembly_final %<>% mutate(process = case_when(betaNTI>2 ~ "Heterogeneous Selection",
                                               betaNTI<(-2) ~ "Homogeneous Selection",
                                               (betaNTI<=2 & betaNTI>=(-2) & raupc>0.95) ~ "Dispersal Limitation",
                                               (betaNTI<=2 & betaNTI>=(-2) & raupc<(-0.95)) ~ "Homogenizing Dispersal",
                                               .default = "Drift"))

# summary
assembly_summary <- assembly_final %>% 
  count(asv_core, same_module, process)

assembly_summary_total <- assembly_summary %>% 
  group_by(asv_core, same_module) %>% 
  summarise(n_total = sum(n))

#plotting
g <- assembly_summary %>% 
  left_join(assembly_summary_total, by=c("asv_core","same_module")) %>% 
  mutate(n_p = n/n_total) %>% 
  ggplot(aes(fill=process, x=same_module, y=n_p)) +
  geom_bar(position="fill", stat="identity") +
  facet_wrap(~asv_core) +
  scale_y_continuous(labels = scales::percent) +
  geom_text(aes(label = paste0(round(n_p*100,1),"%")), 
            position = position_stack(vjust = 0.5), size = 3)+
  theme_bw() +
  theme(axis.text = element_text(size = 11, color = 'black', angle = 90, vjust = 0.5, hjust=1), title = element_text(size = 14),
        strip.text = element_text(size=12, color = 'black'), strip.background = element_rect(color = "grey80", size = 1), 
        panel.grid = element_blank(), panel.background = element_rect(colour = "black")) +
  scale_fill_manual(values=c("#adc178","#d6ccc2","#f07167","#83c5be")) +
  labs(x="", y="Percentage")
print(g)

```


# Normalized Mutual Information (NMI) 

```{r, eval=FALSE, include=FALSE}

nmi_observed_groups <- vector(length=3)
n_shuff <- 10
nmi_shuff_groups <- NULL
for (i in 1:3) {
  data_asv_filtered <- modules_table_three_groups %>% 
    filter(asv_core == core_names[i])
  
  # observed NMI
  hosts <- data_asv_filtered %>% distinct(host_ID, grid, host_group)
  nmi_obs <- aricode::NMI(hosts$grid, hosts$host_group, "sum")
  nmi_observed_groups[i] <- nmi_obs
  
  # shuffle NMI

  for (j in 1:n_shuff) {
    # shuffling the network
    data_asv_mat <- data_asv_filtered %>% select(asv_ID, host_ID, reads) %>% 
    spread(asv_ID, reads, fill = 0) %>% 
    column_to_rownames("host_ID") %>% 
    as.matrix()
    
    null_model <- vegan::nullmodel(data_asv_mat, method = 'c0_samp') 
    shuffled_r <- simulate(null_model, nsim = 1)
  
  # building the network
  network_object <- create_monolayer_network(shuffled_r[,,1], directed = FALSE, bipartite = TRUE, group_names = c("ASV", "Host"))
  
  # modularity analysis
  infomap_object <- run_infomap_monolayer(network_object,
                                          infomap_executable='Infomap',
                                          flow_model = 'undirected',
                                          two_level = TRUE,
                                          silent=TRUE, trials=100, seed=123)
  
  # adding the modules classification to the data
  modules_host <- infomap_object$modules %>% 
    filter(node_group == "Host") %>% 
    dplyr::rename(host_group_shuff = module_level1, host_ID = node_name) %>% 
    mutate(host_ID = as.numeric(host_ID)) %>% 
    left_join(hosts, by="host_ID") %>% 
    filter(!is.na(host_group_shuff))
    
  nmi_shuff <- data.frame(nmi = aricode::NMI(modules_host$grid, modules_host$host_group_shuff, "sum"),
                          asv_core = core_names[i])
  nmi_shuff_groups <- rbind(nmi_shuff_groups, nmi_shuff)
  }
}


# plotting

g <- nmi_shuff_groups %>% 
  ggplot(aes(nmi, fill=asv_core)) + 
  geom_histogram(alpha=0.8, color="white") + 
  facet_wrap(~asv_core, scales="free") +
  scale_y_continuous(limits = c(0, 100)) +
  #geom_vline(data=nmi_observed_three_groups_df, aes(xintercept = nmi), linetype='dashed', color="black") +
  #geom_label(data = nmi_observed_three_groups_df, aes(label = paste("NMI =", round(nmi,2)), x = Inf, y = Inf),
      #       vjust = 1.5,hjust = 1.1, size = 4, color = "black", fill="white") +
  theme_bw() +
  theme(axis.text = element_text(size = 10, color = 'black'), title = element_text(size = 14), strip.text = element_text(size=12), panel.grid = element_blank(), legend.position="none") +
  scale_y_continuous(expand = expansion(mult = c(0.01, 0.01))) +
  scale_fill_manual(values=group.colors) +
  labs(x="Normalized Mutual Information (NMI)", y="Count")
print(g)
cat('\n','\n')


```


```{r, nmi}
cat('## Observed NMI','\n')

# making a data frame out of the shuffled results
# loop for three villages
nmi_observed_df_tv <- NULL
for (j in 1:3) {
  nmi_observed_df <- data_frame(nmi = nmi_observed_three_groups[[j*2]]) %>% 
    mutate(asv_core = core_names[j])
  nmi_observed_df_tv <- rbind(nmi_observed_df_tv, nmi_observed_df)
}

# making a data frame out of the observed results
nmi_observed_three_groups_df <- nmi_observed_three_groups[[1]] %>% 
  bind_rows(nmi_observed_three_groups[[3]]) %>% 
  bind_rows(nmi_observed_three_groups[[5]]) %>% 
  mutate(asv_core = core_names)

g <- nmi_observed_df_tv %>% 
  ggplot(aes(nmi, fill=asv_core)) + 
  geom_histogram(alpha=0.8, color="white") + 
  facet_wrap(~asv_core, scales="free") +
  scale_y_continuous(limits = c(0, 100)) +
  geom_vline(data=nmi_observed_three_groups_df, aes(xintercept = nmi), linetype='dashed', color="black") +
  geom_label(data = nmi_observed_three_groups_df, aes(label = paste("NMI =", round(nmi,2)), x = Inf, y = Inf),
             vjust = 1.5,hjust = 1.1, size = 4, color = "black", fill="white") +
  theme_bw() +
  
  theme(axis.text = element_text(size = 10, color = 'black'), title = element_text(size = 14), strip.text = element_text(size=12), panel.grid = element_blank(), legend.position="none") +
  scale_y_continuous(expand = expansion(mult = c(0.01, 0.01))) +
  scale_fill_manual(values=group.colors) +
  labs(x="Normalized Mutual Information (NMI)", y="Count")
print(g)
cat('\n','\n')

```

# Modules similarity across land use change gradient


## Correlations between variables

Distance is a proxy for habitat degradation.

```{r, variables_correlation}
# reading grid similarity results
grids_similarity_attr <- read_csv("../data/data_processed/village_summary.csv")

# correlations between variables
# loop for three villages
library(psych)
print(psych::pairs.panels(grids_similarity_attr %>% filter(village == vil) %>%  select(-village,-grid1,-grid2), ellipses = F, lm = T))
cat('\n','\n')
```

## Mantel test

```{r, mantel_test}
# mantel test for: modules similarity ~ distance between grids

# distance between grids matrices
load("../data/data_processed/distance_three_villages.RData")
names(grid_distance_three_villages) <- c("Mandena", "Sarahandrano", "Andatsakala")
grid_distance <- grid_distance_three_villages[[vil]]

mantel_result_three_groups <- NULL
final_data_three_groups <- NULL

# loop for three asv groups
for (i in 1:3) {
  
  modules_similarity <- modules_similarity_three_groups[[i]]
  grid_names <- rownames(modules_similarity)
  # filtering the distance matrix to the existing grids
  grid_distance <- grid_distance[grid_names, grid_names]
  
  # calculating mantel test
  mantel_result <- vegan::mantel(modules_similarity, grid_distance)
  mantel_result_df <- data_frame(r = round(mantel_result$statistic,2),
                                 pvalue = round(1-mantel_result$signif,4),
                                 asv_core = core_names[i])
  mantel_result_three_groups <- rbind(mantel_result_three_groups, mantel_result_df)
  
  
  # transforming to long format
  modules_similarity_m <- modules_similarity
  modules_similarity_m[lower.tri(modules_similarity_m)] <- NA
  diag(modules_similarity_m) <- NA
  final_data <- melt(modules_similarity_m) %>% 
    filter(!is.na(value)) %>% 
    mutate(asv_core = core_names[i]) %>% 
    dplyr::rename(grid1 = Var2, grid2 = Var1, module_similarity = value) %>% 
    left_join(grids_similarity_attr %>% filter(village == vil), by=c("grid1","grid2"))
  
  final_data_three_groups <- rbind(final_data_three_groups, final_data)
  
}

# plotting
g <- final_data_three_groups %>% 
  ggplot(aes(y=module_similarity, x=grid_dist, color = asv_core)) +
  geom_point(alpha = 0.8) +
  facet_wrap(~asv_core, scales="free")+
  scale_y_continuous(limits = c(0.0, 0.9)) +
  geom_smooth(method = "glm", se=T, method.args = list(family = "gaussian")) +
  geom_text(data = mantel_result_three_groups, aes(label = paste("r =", r), x = Inf, y = Inf),
            hjust = 1.3, vjust = 1.5, size = 4, color = "black") +
  geom_text(data = mantel_result_three_groups, aes(label = paste("p-value =", pvalue), x = Inf, y = Inf),
            hjust = 1.1, vjust = 3, size = 4, color = "black") +
  theme_bw() +
  theme(axis.text = element_text(size = 10, color = 'black'), title = element_text(size = 14), 
        strip.text = element_text(size=12, color = 'black'), strip.background = element_rect(color = "grey80", size = 1),
        panel.grid = element_blank(), legend.position = "none", panel.border = element_rect(color = "black")) +
  scale_color_manual(values=group.colors) +
  labs(x = "Distance Between Land Uses [m]", y = "Modules Similarity [Bray-Curtis]")
print(g)
cat('\n','\n')
```

# Link prediction 

```{r, eval=FALSE, include=TRUE}
library(caret)
library(yardstick)

# reading the ML results files
test_set <- read_csv("module_prediction/test_set_core.csv")
feature_importance <- read_csv("module_prediction/feature_importance_core.csv")

# setting a threshold for prediction
th <- 0.5
test_set_predict <- test_set %>% 
  select(module, y_proba) %>% 
  mutate(y_proba_th = ifelse(y_proba >= th, 1, 0)) %>% 
  mutate(predict = ifelse(module == y_proba_th, 1, 0)) %>% 
  mutate(y_proba_th = factor(y_proba_th, levels = c(1,0)), module = factor(module, levels = c(1,0)))

cat('## Metrics','\n','\n')

conf_matrix <- caret::confusionMatrix(data=test_set_predict$y_proba_th, reference = test_set_predict$module, dnn = c("Prediction", "Reference"))
print(knitr::kable(conf_matrix$table))
cat('\n','\n')

accuracy <- conf_matrix$overall['Accuracy']
cat("Accuracy: ", round(accuracy, 4), '\n')
precision <- conf_matrix$byClass['Precision']
cat("Precision: ", round(precision, 4), '\n')
recall <- conf_matrix$byClass['Sensitivity']
cat("Recall: ", round(recall, 4), '\n')
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1_score: ", round(f1_score, 4), '\n','\n')

cat('## ROC curve','\n','\n')

auc_roc_score <- yardstick::roc_auc(test_set_predict, module, y_proba)
roc_obj <- yardstick::roc_curve(test_set_predict, module, y_proba) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path(color = "blue") +
  geom_abline(lty = 3) +
  coord_equal() +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))+
  labs(title = "ROC Curve", subtitle = paste("AUC = ", round(auc_roc_score$.estimate,3)))
print(roc_obj)
cat('\n','\n')

cat('## Precision-Recall curve','\n','\n')

auc_pr_score <- yardstick::pr_auc(test_set_predict, module, y_proba)
pr_obj <- yardstick::pr_curve(test_set_predict, module, y_proba) %>% 
  ggplot(aes(x = recall, y = precision)) +
  geom_path(color = "blue") +
  coord_equal() +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))+
  labs(title = "Precision-Recall curve", subtitle = paste("AUC = ", round(auc_pr_score$.estimate,3)))
print(pr_obj)
cat('\n','\n')

cat('## Features importance','\n','\n')

feature_imp_g <- feature_importance %>% 
  arrange(value) %>% 
  mutate(feature = factor(feature, levels = feature)) %>% 
  ggplot(aes(x=feature, y=value)) +
  geom_bar(stat = "identity") +
  coord_flip()+
  scale_y_continuous(limits = c(0, 1)) +
  theme_bw() +
  theme(axis.text = element_text(size = 14, color = 'black'), title = element_text(size = 20), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  labs(x="", y="Importance")
print(feature_imp_g)
cat('\n','\n')
```

```{r, asv_associations, eval=FALSE, include=FALSE}

# taking only ASVs with known Genus
data_tax <- data_asv_tax %>% 
  distinct(host_ID, grid, asv_core, Genus) %>% 
  filter(!is.na(Genus))

# how many hosts in every grid
n_grid_host <- data_asv %>% 
  group_by(grid, asv_core) %>% 
  summarise(n_host = n_distinct(host_ID))

hosts <- data_asv %>% distinct(host_ID, grid, asv_core)

# loop for 3 groups
for(i in core_names) {
  
# calculating observed prevalence
data_tax_pre_obs <- data_tax %>% 
  filter(asv_core == i) %>% 
  count(Genus, asv_core, grid) %>% 
  left_join(n_grid_host, by=c("grid","asv_core")) %>% 
  mutate(prevalence = n/n_host)

summary_shuff <- NULL
for(n in 1:500) {
# shuffle hosts' land use
hosts_shuff <- hosts %>% 
  filter(asv_core == i) %>% 
  mutate(grid = sample(grid)) %>% 
  select(-asv_core)

# calculating shuffeled prevalence
data_tax_pre_shuff <- data_tax %>% 
  filter(asv_core == i) %>%
  select(-grid) %>% 
  left_join(hosts_shuff, by="host_ID") %>% 
  count(Genus, asv_core, grid) %>% 
  left_join(n_grid_host, by=c("grid","asv_core")) %>% 
  mutate(prevalence = n/n_host)

summary_shuff <- rbind(summary_shuff, data_tax_pre_shuff)
}

# calculating mean and sd
results_summary <- summary_shuff %>% 
  group_by(Genus, grid) %>% 
  summarise(mean = mean(prevalence), sd = sd(prevalence))

# calculating z-scores and p-values
summary_z <- data_tax_pre_obs %>% 
  left_join(results_summary, by=c("Genus","grid")) %>% 
  mutate(z = (prevalence-mean)/sd) %>% 
  mutate(p = 2 * pnorm(-abs(z))) %>% 
  mutate(p_c = p.adjust(p, method = "BH")) %>% 
  mutate(sig = case_when((z>0 & p_c<0.05) ~ T, .default = F))
}
```

```{r, prediction, eval=FALSE, include=TRUE}
library(caret)
library(yardstick)
library(randomForest)
library(ROSE)

# Assume df is your dataframe and "group" is the binary target column
df <- read_csv("../data/data_processed/ML_module/ML_rattus_three_villages_non_core.csv") %>% 
  select(-host_ID.x,-host_ID.y) %>% 
  mutate(module=as.factor(module), grid=as.factor(grid), sex=as.factor(sex), season=as.factor(season))

# Set seed for reproducibility
set.seed(123)

# Define the control for cross-validation
control <- trainControl(method = "cv", number = 5)

# Create a function for under-sampling
undersample <- function(df) {
  # Separate features and target
  features <- df[, -which(names(df) == "module")]
  target <- df$module
  
  # Combine into a new dataframe
  data <- cbind(features, module = target)
  
  # Perform under-sampling using ROSE package
  balanced_data <- ovun.sample(module ~ ., data = data, method = "over", seed = 1)$data
  
  return(balanced_data)
}

# Apply under-sampling to the dataframe
balanced_df <- df

# Separate features and target for the balanced dataset
X <- balanced_df[, -which(names(balanced_df) == "module")]
y <- as.factor(balanced_df$module)

# Define the training model using random forest
rf_model <- caret::train(X, y,
                  method = "rf",
                  trControl = control,
                  metric = "Accuracy")

# Print the model details
print(rf_model)

# Evaluate the model
predictions_prob <- predict(rf_model, X, type = "prob")[,2]
predictions_class <- predict(rf_model, X)

# Convert predictions to a data frame for yardstick
results <- data.frame(
  truth = y,
  predicted_prob = predictions_prob,
  predicted_class = predictions_class
)

# Calculate precision
precision_val <- precision(results, truth = truth, estimate = predicted_class)
print(paste("Precision_val:", precision_val$.estimate))

# Calculate recall
recall_val <- recall(results, truth = truth, estimate = predicted_class)
print(paste("Recall:", recall_val$.estimate))

# Calculate F1 score
f1_val <- f_meas(results, truth = truth, estimate = predicted_class)
print(paste("F1 Score:", f1_val$.estimate))

# Plot ROC Curve
auc_roc_score <- yardstick::roc_auc(results, truth, predicted_prob)
roc_obj <- yardstick::roc_curve(results, truth, predicted_prob) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path(color = "blue") +
  geom_abline(lty = 3) +
  coord_equal() +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))+
  labs(title = "ROC Curve", subtitle = paste("AUC = ", round(auc_roc_score$.estimate,3)))
print(roc_obj)
cat('\n','\n')


# Plot Precision-Recall Curve
auc_pr_score <- yardstick::pr_auc(results, truth = truth, predicted_prob)
pr_obj <- yardstick::pr_curve(results, truth = truth, predicted_prob) %>% 
  ggplot(aes(x = recall, y = precision)) +
  geom_path(color = "blue") +
  coord_equal() +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))+
  labs(title = "Precision-Recall curve", subtitle = paste("AUC = ", round(auc_pr_score$.estimate,3)))
print(pr_obj)
cat('\n','\n')

# Feature importance analysis
# Extract the final random forest model
final_rf <- rf_model$finalModel

# Calculate feature importance
importance <- importance(final_rf)
var_importance <- data.frame(Variables = rownames(importance), 
                             Importance = importance[, 1])

# Plot feature importance
ggplot(var_importance, aes(x = reorder(Variables, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ggtitle("Feature Importance") +
  xlab("Variables") +
  ylab("Importance (Mean Decrease in Gini)") +
  theme_minimal()
```


